{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 3\n",
    "## Exercise 5\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### Imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "### Task 1:\n",
    "\n",
    "> Load the trajectories and perform MC evaluation for $v_{\\pi}(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mountain.envs import MountainEnv\n",
    "\n",
    "# Initialize Mountain environment (v1 / Mountain Lite)\n",
    "env = MountainEnv()\n",
    "state = env.reset()  # starting at row 15, col 0 (near top-left)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 trajectories from SpaceY's policy π.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"trajectories.pickle\", \"rb\") as f:\n",
    "    trajectories = pickle.load(f)\n",
    "print(f\"Loaded {len(trajectories)} trajectories from SpaceY's policy π.\")\n",
    "# (Each trajectory is a list of (state, action, reward) tuples for one episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     36\u001b[0m     action \u001b[38;5;241m=\u001b[39m epsilon_greedy_action(state, Q, epsilon)\n\u001b[0;32m---> 37\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     38\u001b[0m     episode_experience\u001b[38;5;241m.\u001b[39mappend((state, action, reward))\n\u001b[1;32m     39\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize action-value table Q and visit counts\n",
    "n_actions = 5\n",
    "Q = np.zeros((env.height, env.width, n_actions))\n",
    "returns_count = np.zeros((env.height, env.width, n_actions))\n",
    "\n",
    "def epsilon_greedy_action(state, Q, epsilon):\n",
    "    \"\"\"Select an action using epsilon-greedy strategy based on Q.\"\"\"\n",
    "    r, c = state\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Exploration: random action\n",
    "        return np.random.randint(0, n_actions)\n",
    "    else:\n",
    "        # Exploitation: choose best known action (break ties randomly)\n",
    "        values = Q[r, c]\n",
    "        max_val = np.max(values)\n",
    "        best_actions = np.flatnonzero(np.isclose(values, max_val))\n",
    "        return np.random.choice(best_actions)\n",
    "\n",
    "# Monte Carlo Control parameters\n",
    "num_episodes = 10000\n",
    "min_epsilon = 0.1  # minimum exploration rate\n",
    "epsilon = 1.0      # start fully random\n",
    "\n",
    "for episode in range(1, num_episodes+1):\n",
    "    # Decay epsilon over time (ensure eventually policy is greedy in the limit)\n",
    "    # For example, linear decay:\n",
    "    epsilon = max(min_epsilon, 1 - episode/num_episodes)  \n",
    "    episode_experience = []  # to store (state, action, reward) tuples\n",
    "    \n",
    "    # Generate one episode\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = epsilon_greedy_action(state, Q, epsilon)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        episode_experience.append((state, action, reward))\n",
    "        state = next_state\n",
    "    \n",
    "    # Calculate returns backwards and update Q-values\n",
    "    G = 0.0\n",
    "    visited = set()  # track state-action pairs visited in this episode\n",
    "    # Traverse episode in reverse to compute returns\n",
    "    for state, action, reward in reversed(episode_experience):\n",
    "        G += reward  # accumulate return\n",
    "        if (state, action) not in visited:  # first-visit\n",
    "            visited.add((state, action))\n",
    "            r, c = state\n",
    "            returns_count[r, c, action] += 1\n",
    "            # Incremental MC update toward return G\n",
    "            Q[r, c, action] += (G - Q[r, c, action]) / returns_count[r, c, action]\n",
    "    # (Policy implicitly improves as Q is updated and epsilon-greedy uses new Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
